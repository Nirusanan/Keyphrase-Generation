converted_ner_data = [
    {
         'text': 'We demonstrate that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even becoming competitive with prior state-ofthe-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive\nlanguage model with 175 billion parameters, 10x more than any previous nonsparse language model, and test its performance in the few-shot setting. For all\ntasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks\nand few-shot demonstrations specified purely via text interaction with the model.\nGPT-3 achieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks. We also identify some datasets where GPT3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces\nmethodological issues related to training on large web corpora.',
         'entities': [(31, 46, 'keywords'), (398, 403, 'keywords'), (595, 598, 'keywords'), (713, 730, 'keywords')]
    },
    {
         'text': 'NLP has shifted from learning task-specific representations and designing task-specific architectures\nto using task-agnostic pre-training and task-agnostic architectures. This shift has led to substantial\nprogress on many challenging NLP tasks such as reading comprehension, question answering, textual\nentailment, among others. Even though the architecture and initial representations are now taskagnostic,\na final task-specific step remains: fine-tuning on a large dataset of examples to adapt a task\nagnostic model to perform a desired task.',
         'entities': [(142, 155, 'keywords'), (234, 237, 'keywords'), (444, 455, 'keywords'), (461, 474, 'keywords')]
    },
    {
         'text': 'The dataset and model size are about two orders of magnitude larger than those used for GPT-2,\nand include a large amount of Common Crawl, creating increased potential for contamination and\nmemorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does\nnot overfit its training set by a significant amount, measured relative to a held-out validation set with\nwhich it was deduplicated. For each benchmark, we produce a ‘clean’ version which removes all\npotentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything\nin the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). We\nthen evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on\nthe clean subset is similar to the score on the entire dataset, this suggests that contamination, even if\npresent, does not have a significant effect on reported results. In most cases performance changes only\nnegligibly, and we see no evidence that contamination level and performance difference are correlated.\nWe conclude that either our conservative method substantially overestimated contamination or that\ncontamination has little effect on performance. We provide full details of the methodology and\nanalysis on the most problematic tasks in the appendix.',
         'entities': [(88, 93, 'keywords'), (271, 281, 'keywords'), (701, 706, 'keywords'), (762, 767, 'keywords')]
    },
    {
         'text': 'We presented a 175 billion parameter language model which shows strong performance on many\nNLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly\nmatching the performance of state-of-the-art fine-tuned systems, as well as generating high-quality\nsamples and strong qualitative performance at tasks defined on-the-fly. We documented roughly\npredictable trends of scaling in performance without using fine-tuning. We also discussed the social\nimpacts of this class of model. Despite many limitations and weaknesses, these results suggest that\nvery large language models may be an important ingredient in the development of adaptable, general\nlanguage systems.',
         'entities': [(91, 94, 'keywords'), (123, 132, 'keywords'), (134, 142, 'keywords'), (148, 156, 'keywords'),
                      (588, 609, 'keywords')]
    },
    {
         'text': 'Malicious uses of language models can be somewhat difficult to anticipate because they often\ninvolve repurposing language models in a very different environment or for a different purpose than\nresearchers intended. To help with this, we can think in terms of traditional security risk assessment\nframeworks, which outline key steps such as identifying threats and potential impacts, assessing\nlikelihood, and determining risk as a combination of likelihood and impact. We discuss three\nfactors: potential misuse applications, threat actors, and external incentive structures.',
         'entities': [(0, 9, 'keywords'), (113, 128, 'keywords'), (280, 295, 'keywords'), (352, 359, 'keywords')]
    },
    {
         'text': 'Ease of use is another significant incentive. Having stable infrastructure has a large impact on the\nadoption of TTPs. The outputs of language models are stochastic, however, and though developers\ncan constrain these (e.g. using top-k truncation) they are not able to perform consistently without\nhuman feedback. If a social media disinformation bot produces outputs that are reliable 99% of the\ntime, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor\nrequired in operating this bot. But a human is still needed to filter the outputs, which restricts how\nscalable the operation can be.',
         'entities': [(113, 117, 'keywords'), (134, 149, 'keywords'), (297, 311, 'keywords'), (318, 330, 'keywords')]
    },
    {
         'text': 'Recall that we conduct our experiments with training examples from FewGLUE, a randomly selected\nsubset of the original SuperGLUE training examples. We used a fixed random seed s0 to generate\nFewGLUE. Let Σi be the randomly selected subset of SuperGLUE for random seed si\n, so Σ0 =FewGLUE. In this subsection, we create two additional subsets of SuperGLUE, Σ1 and Σ2, based\non different seeds. This allows us to investigate\nhow different sets of training examples affect performance. To this end, we run PET for CB, RTE\nand MultiRC using the three Σi. To measure only the effect of varying the training set while ignoring\nunlabeled examples, we do not use distillation',
         'entities': [(67, 74, 'keywords'), (119, 128, 'keywords'), (621, 639, 'keywords'), (655, 667, 'keywords')]
    },
    {
         'text': 'In this section, we show that briefly finetuning on instructions data rapidly leads to improvements\non MMLU. Although the non-finetuned version of LLaMA-65B is already able to follow basic instructions, we observe that a very small amount of\nfinetuning improves the performance on MMLU, and further improves the ability of the model to\nfollow instructions. Since this is not the focus of this paper, we only conducted a single experiment\nfollowing the same protocol to train an instruct model, LLaMA-I.',
         'entities': [(103, 107, 'keywords'), (147, 156, 'keywords'), (478, 492, 'keywords'), (494, 501, 'keywords')]
    },
    {
         'text': 'While there are millions of potential functions to curate problems from, we only collected about 40,000 because\nnot all functions accept inputs and return outputs. Even when they do, most objects captured at runtime cannot be\npickled and restored outside the sandbox unless the project was installed.\nSince our tracing methodology produced inputs and outputs for all invoked functions, even builtin and library calls imported by the project were turned into problems. For this\nreason, functions from tracing tended to be the building blocks of command-line utilities. To excel at these tasks,\nthe model does not need to know advanced algorithms and data structures. Rather, it needs to be able to follow instructions to implement the functionality specified in the\ndocstring. Thus, tracing complements the puzzle nature of coding competition problems and broadens the distribution of tasks.',
         'entities': [(525, 540, 'keywords'), (634, 644, 'keywords'), (649, 664, 'keywords'), (704, 716, 'keywords')]
    },
    {
         'text': 'In the previous sections, we presented two methods we used to automatically create training problems. However,\nit is unclear how to control for quality. Some prompts underspecify the function that is implemented, in which\ncase a perfectly valid solution may be wrongly penalized by the unit test. Some problems are stateful, and subsequent\nexecutions can result in different outcomes. To address these issues, we use Codex-12B to generate 100\nsamples per curated problem. If no samples pass the unit tests, we consider the task to be either ambiguous or too\ndifficult, and filter it out. We reran this verification several times to remove stateful or non-deterministic problems.',
         'entities': [(158, 165, 'keywords'), (286, 295, 'keywords'), (417, 426, 'keywords'), (443, 450, 'keywords')]
    },
    {
         'text': 'Generating code from docstrings is possible with Codex because code typically follows after a docstring, but it is not\neasy to induce Codex to generate docstrings from code. Nevertheless, we are motivated to produce a docstring writing\nmodel for safety reasons, as such a model can be used to describe the intent behind generated code. Using the training\nproblems described in the previous section, we can easily create a training dataset for code-conditional docstring\ngeneration.\nSpecifically, for each training problem, we assemble a training example by concatenating the function signature, the\nreference solution, and then the docstring. Just as we train\nCodex-S by minimizing negative log-likelihood of the reference solution, we train the docstring generating models\nCodex-D by minimizing negative log-likelihood of the docstring.\nWhen we benchmark our code generation models, we measure pass on the HumanEval dataset, where correctness\nis defined by passing a set of unit tests. However, there is no similar way to evaluate docstring samples automatically.\nTherefore, we grade sample docstrings by hand, considering a docstring correct if it uniquely and accurately specifies\nthe code body. Due to the time consuming nature of this process, we only grade 10 samples per problem, for a total\nof 1640 problems, from Codex-D-12B at temperature 0.8.',
         'entities': [(134, 139, 'keywords'), (422, 438, 'keywords'), (860, 875, 'keywords'), (975, 985, 'keywords')]
    },
    {
         'text': 'One of the key risks associated with using code generation models in practice is over-reliance on generated outputs.\nDue to the limitations described above as well as alignment issues described below, Codex may suggest solutions that\nsuperficially appear correct but do not actually perform the task the user intended. This could particularly affect novice\nprogrammers, and could have significant safety implications depending on the context. We discuss a related issue in\nAppendix G, namely that code generation models can suggest insecure code. For these reasons, human oversight and\nvigilance is required for safe use of code generation systems like Codex.\nWe note several immediate ways to improve safety in the subsection on risk mitigation below, though over-reliance\nin particular is one that we believe merits further inquiry in industry and academia.\nWhile it is conceptually straight forward to provide documentation to users reminding them\nabout model limitations, empirical investigation is necessary in order to identify how to reliably ensure vigilance in\npractice across a range of user experience levels, UI designs,\nand tasks. One challenge researchers should consider is that\nas capabilities improve, it may become increasingly difficult to guard against “automation bias.”',
         'entities': [(201, 206, 'keywords'), (497, 519, 'keywords'), (730, 745, 'keywords'), (957, 974, 'keywords'),
                      (1274, 1289, 'keywords')]
    },
    {
         'text': 'In this paper we will discuss pandas, a Python library of rich\ndata structures and tools for working with structured data sets common to\nstatistics, finance, social sciences, and many other fields. The library provides\nintegrated, intuitive routines for performing common data manipulations and\nanalysis on such data sets. It aims to be the foundational layer for the future of\nstatistical computing in Python. It serves as a strong complement to the existing\nscientific Python stack while implementing and improving upon the kinds of data\nmanipulation tools found in other statistical programming languages such as R.\nIn addition to detailing its design and features of pandas, we will discuss\nfuture avenues of work and growth opportunities for statistics and data analysis\napplications in the Python language.',
         'entities': [(30, 36, 'keywords'), (40, 46, 'keywords'), (63, 78, 'keywords'), (378, 399, 'keywords'),
                      (586, 607, 'keywords')]
    },
    {
         'text': 'The pandas library, under development since 2008, is\nintended to close the gap in the richness of available data\nanalysis tools between Python, a general purpose systems\nand scientific computing language, and the numerous domain specific statistical computing platforms and database languages.\nWe not only aim to provide equivalent functionality\nbut also implement many features, such as automatic data\nalignment and hierarchical indexing, which are not readily\navailable in such a tightly integrated way in any other libraries\nor computing environments to our knowledge. While initially\ndeveloped for financial data analysis applications, we hope that\npandas will enable scientific Python to be a more attractive\nand practical statistical computing environment for academic\nand industry practitioners alike. The library’s name derives\nfrom panel data, a common term for multidimensional data\nsets encountered in statistics and econometrics.',
         'entities': [(4, 10, 'keywords'), (136, 142, 'keywords'), (274, 292, 'keywords'), (388, 402, 'keywords'),
                      (602, 625, 'keywords')]
    },
    {
         'text': 'The result of the pivot operation has a hierarchical index\nfor the columns. As we will show in a later section, this is\na powerful and flexible way of representing and manipulating multidimensional data. Currently the pivot method of\nDataFrame only supports pivoting on two columns to reshape\nthe data, but could be augmented to consider more than just\ntwo columns. By using hierarchical indexes, we can guarantee\nthat the result will always be two-dimensional. Later in the\npaper we will demonstrate the pivot_table function which\ncan produce spreadsheet-style pivot table data summaries as\nDataFrame objects with hierarchical rows and columns.\nBeyond observational data, one will also frequently encounter categorical data, which can be used to partition identifiers into broader groupings. For example, stock tickers might\nbe categorized by their industry or country of incorporation.\nHere we have created a DataFrame object cats storing\ncountry and industry classifications for a group of stocks.',
         'entities': [(18, 23, 'keywords'), (181, 202, 'keywords'), (375, 395, 'keywords'), (708, 724, 'keywords')]
    },
    {
         'text': 'ChatGPT has revolutionized many research and industrial fields.\nChatGPT has shown great potential in software engineering to\nboost various traditional tasks such as program repair, code understanding, and code generation. However, whether automatic\nprogram repair (APR) applies to deep learning (DL) programs is\nstill unknown. DL programs, whose decision logic is not explicitly\nencoded in the source code, have posed unique challenges to APR.\nWhile to repair DL programs, an APR approach needs to not only\nparse the source code syntactically but also needs to understand the\ncode intention. With the best prior work, the performance of fault\nlocalization is still far less than satisfactory (only about 30%). Therefore, in this paper, we explore ChatGPT’s capability for DL program\nrepair by asking three research questions. (1) Can ChatGPT debug\nDL programs effectively? (2) How can ChatGPT’s repair performance be improved by prompting? (3) In which way can dialogue\nhelp facilitate the repair? On top of that, we categorize the common\naspects useful for prompt design for DL program repair. Also, we\npropose various prompt templates to facilitate the performance\nand summarize the advantages and disadvantages of ChatGPT’s\nabilities such as detecting bad code.',
         'entities': [(0, 7, 'keywords'), (239, 263, 'keywords'), (281, 294, 'keywords'), (1058, 1071, 'keywords')]
    },
    {
         'text': 'To evaluate the automatic DL program repair ability of ChatGPT, we use a benchmark released by a recent study, which contains\n58 buggy DL programs collected from StackOverflow and Github. In particular, each buggy DL program contains up to five types of\nfaults, including inappropriate hyperparameter settings, incorrect data preprocessing, and incorrect model architectures.\nIn the evaluation, we use the buggy programs collected from StackOverflow, because the programs in GitHub exceed the maximum input tokens.\nRecent studies explore the program repair ability of ChatGPT on conventional programs using different benchmarks.\nTable 1 lists the differences between these benchmarks. The two benchmarks consist of Python/Java programs implementing fundamental\nfunctionalities such as sorting algorithms. Each program’s average number of lines ranges from 13 to 22 and implements\none functionality. For comparison, in this paper, we use the benchmark in which every program contains about 46 lines on average,\nimplementing several functionalities including data preprocessing, DL model construction, model training, and evaluation.\nFurthermore, programs in the prior two benchmarks involve few dependencies, while the benchmark in this paper involves more than 6\ndependencies for each program, including TensorFlow, Keras, and PyTorch. We hope our study could complement prior work\nby exploring ChatGPT’s repair ability on more complicated programs and exploring the capabilities of handling dependency issues.',
         'entities': [(26, 28, 'keywords'), (568, 575, 'keywords'), (1057, 1075, 'keywords'), (1304, 1314, 'keywords'),
                      (1316, 1321, 'keywords'), (1327, 1334, 'keywords')]
    },
    {
         'text': 'We adopt the evaluation metrics following existing works. In the fault detection evaluation, we consider an alarm\nraised by an approach correct if it is made on a buggy program,\nregardless of whether the root cause has been identified. Due to the\nstochastic nature of ChatGPT, for each buggy program, we repeat\nthe fault detection request five times, each with a new conversation.\nIf the majority (≥ 3) of replies correctly report fault existence,\nwe denote the result as a Yes; otherwise, No. A fault localization\nis considered correct if the fault reported is the same as the one\nmarked in the benchmark. Since there could be more than\none fault in a buggy program, we record the number of faults\nreported by each approach. For ChatGPT, we put down the number\nof faults reported each round, and we use the best result among\nthe five rounds for comparison. In the repair evaluation, a repair\nis considered correct if the concerned fault is repaired in the same\nway as that in the benchmark. Note that we examine whether a\nrepair is heading in the correct direction for the fault concerning\nincorrect training epochs. For example, for a buggy program, the\ntraining epoch is repaired from 1 to 50 in the benchmark, while\nChatGPT repairs it to 20. Although the repaired values of epochs\nare different, we consider it a correct repair since the repairing strategies.',
         'entities': [(13, 31, 'keywords'), (65, 80, 'keywords'), (268, 275, 'keywords'), (981, 990, 'keywords'),
                      (1101, 1116, 'keywords')]
    },
    {
         'text': 'To debug and repair DL programs, various approaches have been\nproposed. AutoTrainer proposed an automatic approach to\ndetect and fix the training problems in DNN programs at runtime.\nIt particularly focuses on detecting and repairing five common\ntraining problems: gradient vanish, gradient explode, dying ReLU,\noscillating loss, and slow convergence. It encapsulates and automates the detecting and repairing process by dynamic monitoring.\nAmazon SageMaker Debugger and UMLAUT provide a\nset of built-in heuristics to debug faults in DNN models during\ntraining. DeepLocalize, the first fault localization approach\nfor DNN models (such as incorrect learning rate or inappropriate\nloss function). It locates in layers where the symptom happens\ninstead of where the fault’s root cause resides. DeepFD and\nDeepDiagnosis diagnose and localize faults in DL programs.\nDeepDiagnosis also suggests actionable advice on how to repair.',
         'entities': [(265, 280, 'keywords'), (282, 298, 'keywords'), (300, 310, 'keywords'), (618, 628, 'keywords'),
                      (802, 815, 'keywords')]
    },
    {
         'text': 'AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) trained on broad\ndata (generally using self-supervision at scale) that can be adapted to a wide range of downstream tasks.\nWe call these models foundation models to underscore their critically central yet incomplete character.\nThis report provides a thorough account of the opportunities and risks of foundation models, ranging\nfrom their capabilities (e.g., language, vision, robotic manipulation, reasoning, human interaction) and\ntechnical principles (e.g., model architectures, training procedures, data, systems, security, evaluation,\ntheory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse,\neconomic and environmental impact, legal and ethical considerations). Though foundation models are\nbased on standard deep learning and transfer learning, their scale results in new emergent capabilities,\nand their effectiveness across so many tasks incentivizes homogenization. Homogenization provides\npowerful leverage but demands caution, as the defects of the foundation model are inherited by all the\nadapted models downstream. Despite the impending widespread deployment of foundation models,\nwe currently lack a clear understanding of how they work, when they fail, and what they are even\ncapable of due to their emergent properties. To tackle these questions, we believe much of the critical\nresearch on foundation models will require deep interdisciplinary collaboration commensurate with\ntheir fundamentally sociotechnical nature.',
         'entities': [(0, 2, 'keywords'), (125, 141, 'keywords'), (191, 207, 'keywords'), (853, 866, 'keywords'),
                      (871, 888, 'keywords')]
    },
    {
         'text': 'Most AI systems today are powered by machine learning, where predictive\nmodels are trained on historical data and used to make future predictions. The rise of machine\nlearning within AI started in the 1990s, representing a marked shift from the way AI systems were\nbuilt previously: rather than specifying how to solve a task, a learning algorithm would induce\nit based on data — i.e., the how emerges from the dynamics of learning. Machine learning also\nrepresented a step towards homogenization: a wide range of applications could now be powered\nby a single generic learning algorithm such as logistic regression.\nDespite the ubiquity of machine learning within AI, semantically complex tasks in natural language processing (NLP) and computer vision such as question answering or object recognition,\nwhere the inputs are sentences or images, still required domain experts to perform “feature engineering” — that is, writing domain-specific logic to convert raw data into higher-level features\nthat were more suitable for popular machine learning methods.',
         'entities': [(5, 7, 'keywords'), (159, 175, 'keywords'), (595, 614, 'keywords'), (698, 725, 'keywords'),
                      (736, 751, 'keywords'), (886, 905, 'keywords')]
    },
    {
         'text': 'Foundation models have taken shape most strongly in NLP, so we focus our\nstory there for the moment. That said, much as deep learning was popularized in computer vision\nbut exists beyond it, we understand foundation models as a general paradigm of AI, rather than\nspecific to NLP in any way. By the end of 2018, the field of NLP was about to undergo another\nseismic change, marking the beginning of the era of foundation models. On a technical level,\nfoundation models are enabled by transfer learning and scale. The idea of transfer\nlearning is to take the “knowledge” learned from one task (e.g., object recognition in images) and\napply it to another task (e.g., activity recognition in videos). Within deep learning, pretraining is\nthe dominant approach to transfer learning: a model is trained on a surrogate task (often just as a\nmeans to an end) and then adapted to the downstream task of interest via fine-tuning.\nTransfer learning is what makes foundation models possible, but scale is what makes them\npowerful. Scale required three ingredients: (i) improvements in computer hardware — e.g., GPU\nthroughput and memory have increased 10× over the last four years (§4.5: systems); (ii) the\ndevelopment of the Transformer model architecture that leverages the parallelism of the hardware\nto train much more expressive models; and (iii) the availability of much more training data.',
         'entities': [(52, 55, 'keywords'), (120, 133, 'keywords'), (153, 168, 'keywords'), (248, 250, 'keywords'),
                      (484, 501, 'keywords'), (876, 891, 'keywords'), (1215, 1226, 'keywords')]
    },
    {
         'text': 'Additionally, while many of the iconic foundation models at the time of writing are language\nmodels, the term language model is simply too narrow for our purpose: as we describe, the scope of\nfoundation models goes well beyond language. We also considered terms such as general-purpose\nmodel and multi-purpose model that capture the important aspect that these models can serve\nmultiple downstream tasks, but both fail to capture their unfinished character and the need for\nadaptation. Terms such as task-agnostic model would capture the manner of training, but fail to\ncapture the significant implication to downstream applications.\nWe chose the new term foundation models to identify the models and the emerging paradigm that\nare the subject of this report. In particular, the word “foundation” specifies the role these models\nplay: a foundation model is itself incomplete but serves as the common basis from which many\ntask-specific models are built via adaptation. We also chose the term “foundation" to connote the\nsignificance of architectural stability, safety, and security: poorly-constructed foundations are a\nrecipe for disaster and well-executed foundations are a reliable bedrock for future applications. At\npresent, we emphasize that we do not fully understand the nature or quality of the foundation that\nfoundation models provide; we cannot characterize whether the foundation is trustworthy or not.\nThus, this is a critical problem for researchers, foundation model providers, application developers\nwho rely on foundation models, policymakers, and society at large to address.',
         'entities': [(110, 124, 'keywords'), (387, 403, 'keywords'), (500, 513, 'keywords'), (656, 673, 'keywords')]
    },
    {
         'text': 'The political economy in which foundations models are designed, developed, and\ndeployed provides an inevitable incentive structure for decision-making at every stage. How\npeople and institutions respond to incentives is an elementary lesson of economics. Market-driven\ncommercial incentives can align well with social benefit: making foundation models more accurate,\nreliable, safe, and efficient while searching for a wide variety of potential use cases can produce a\ngreat deal of social utility. However, commercial incentives can also lead to market failures and\nunderinvestment in domains where shareholders are unable to capture the value of innovation.\nJust as the pharmaceutical industry has little incentive to devote significant resources to the\nresearch and development of malaria treatments, because poor people cannot afford medications,\nthe tech industry has little incentive to devote significant resources to technologies designed\nfor improving the condition of poor and marginalized people. What’s more, commercial incentives can lead companies to ignore\nsocial externalities such as the technological displacement of labor,\nthe health of an informational ecosystem required for democracy, the environmental cost of computing resources, and the profit-driven sale\nof technologies to non-democratic regimes. Finally, there is little incentive for any given company\nto create an open, decentralized ecosystem for developing foundation models that encourages broad participation.',
         'entities': [(4, 21, 'keywords'), (31, 49, 'keywords'), (135, 150, 'keywords'), (1262, 1275, 'keywords'),
                      (1400, 1423, 'keywords')]
    },
    {
         'text': 'There are tremendous economic incentives to push the capabilities and scale of foundation models,\nso we anticipate steady technological progress over the coming years. But the\nsuitability of a technology relying largely on emergent behavior for widespread deployment to\npeople is unclear. What is clear that we need to be cautious, and that now is the time to establish\nthe professional norms that will enable the responsible research and deployment of foundation\nmodels. Academia and industry need to collaborate on this: industry ultimately makes concrete\ndecisions about how foundation models will be deployed, but we should also lean on academia,\nwith its disciplinary diversity and non-commercial incentives around knowledge production and\nsocial benefit, to provide distinctive guidance on the development and deployment of foundation\nmodels that is both technically and ethically grounded.',
         'entities': [(79, 96, 'keywords'), (122, 144, 'keywords'), (687, 712, 'keywords'), (720, 740, 'keywords')]
    },
    {
         'text': 'Reasoning and search problems such as theorem proving and program synthesis have been long-standing challenges in AI.\nThe combinatorial search space renders traditional search-based methods intractable. However, humans are known to operate intuitively\neven in the most mathematical of domains, and indeed existing work\nsuch as AlphaGo have already shown that deep neural networks can be effective in guiding the\nsearch space. But humans also transfer knowledge across tasks, facilitating much more efficient\nadaptation and the ability to reason more abstractly. Foundation models offer the possibility of\nclosing this gap: their multi-purpose nature along with their strong generative and multimodal\ncapabilities offer new leverage for controlling the combinatorial explosion inherent to search.',
         'entities': [(114, 116, 'keywords'), (327, 334, 'keywords'), (359, 379, 'keywords'), (689, 699, 'keywords')]
    },
    {
         'text': 'Education is a complex and subtle domain; effective teaching involves reasoning\nabout student cognition and should reflect the learning goals of students. The nature of foundation\nmodels presents promise here that has yet to be realized in the sphere of AI for education: while\ncertain many streams of data in education are individually too limited to train foundation models,\nthe ability to leverage relevant data from outside the domain (e.g., the Internet) and make use\nof data across multiple modalities (e.g., textbooks, mathematical formula, diagrams, video-based\ntutorials) jointly offers hope for foundation models that are broadly applicable to educational tasks.\nIf foundation models lead to a significant improvement in education-relevant capabilities, there\nis clear potential for new applications that align with the open-ended generative (e.g., problem\ngeneration) and interactive (e.g., feedback to teachers) aspects of foundation models; the sample\nefficient adaptation of foundation models suggests greater ability for adaptive and personalized\nlearning. In this event, renewed consideration is required of hallmarks of applying technology to\neducation (e.g., student privacy), along with certain concerns becoming more critical (e.g., inequity\nin access to technology in education, technology-aided plagiarism).',
         'entities': [(70, 79, 'keywords'), (254, 256, 'keywords'), (605, 622, 'keywords'), (1160, 1169, 'keywords')]
    },
    {
         'text': 'Security and privacy for foundation models is largely uncharted at\npresent. Fundamentally, foundation models are a high-leverage single point of failure, making\nthem a prime target for attack: existing work demonstrates a variety of security vulnerabilities\n(e.g., adversarial triggers to generate undesirable outputs) or privacy risks (e.g., memorization of\ntraining data) for these models. Further, the generality of foundation models compounds these\nconcerns, intensifying the risk for function creep or dual use (i.e., use for unintended purposes). For\nsecurity, we view foundation models as akin to operating systems in traditional software systems;\nwe discuss steps towards secure foundation models which, if achieved, would provide a strong\nabstraction layer to build upon for reliable ML applications. For privacy, by leveraging knowledge\ntransfer from public data, foundation models may enable more sample efficient adaptation to\nsensitive data distributions, i.e., privacy-preserving applications may incur less degradation in\naccuracy when built using foundation models.',
         'entities': [(25, 42, 'keywords'), (557, 565, 'keywords'), (793, 808, 'keywords'), (939, 953, 'keywords')]
    },
    {
         'text': 'In many contexts, machine learning has been shown to contribute\nto, and potentially amplify, societal inequity. Foundation models may extend this trend, i.e., furthering the unjust treatment of people who have been historically discriminated against. However,\nunderstanding the relationship between inequity and foundation models requires reckoning with\nthe abstraction of foundation models; foundation models are intermediary assets that are adapted\nfor applications that impact users. Therefore, we delineate intrinsic biases, i.e., properties in foundation models that portend harm, and extrinsic harms, i.e., harms arising in the context of specific\napplications built using foundation models. We taxonomize various sources (e.g., training data, lack\nof diversity among foundation model developers, the broader sociotechnical context) that give rise\nto these biases and harms, emphasizing the importance, and technical difficulty, of source tracing to\nunderstand ethical and legal responsibility. We do not view unfairness as inevitable in the foundation model paradigm: to address unfair outcomes that arise from foundation models, we dually\nconsider proactive interventions (e.g., technical methods like counterfactual data augmentation)\nand reactive recourse.',
         'entities': [(18, 34, 'keywords'), (312, 329, 'keywords'), (735, 748, 'keywords'), (1225, 1242, 'keywords')]
    },
    {
         'text': 'A longstanding challenge of robotics research is to endow robots with the ability to handle the\nmyriad conditions they will encounter in real-world settings. In this section, we discuss how the\nideas underlying foundation models can potentially help bring about “generalist” robots that can,\nfor example, cook a new meal in a new house, with a new kitchen. To make progress towards\nthis goal, existing foundation models will not suffice. We need new types of models trained on\na multitude of data sources, spanning grounded robotic interaction data to videos of humans\nperforming tasks, amongst others. We focus on how such foundation models can apply to the\nproblem of a robot controlling its own physical embodiment to successfully perform different tasks.\nThis is a high-dimensional and closed-loop decision-making problem: the actions that a robot\ntakes directly influence what it perceives next, which in turn influences the next robot action.\nThis closed-loop aspect is not traditionally studied in language and computer vision, where large\noffline datasets are dominant and foundation models have already seen success. We focus on how\nthe demonstrated benefits of foundation models large-scale, self-supervised learning can be\nleveraged in this new closed-loop data regime. The promise of a new type of robotic foundation\nmodel is in its ability to amplify the potential of robots to improve key facets of daily life ranging\nfrom manufacturing, construction, autonomous driving, to household aid and personal assistance, amongst others.\nOur discussion in this section primarily focuses on mobile manipulation robots for household tasks, but\nwe expect its essence to be broadly applicable to the other use-cases of robotics listed above.',
         'entities': [(28, 36, 'keywords'), (211, 228, 'keywords'), (1018, 1033, 'keywords'), (1202, 1226, 'keywords'),
                      (1627, 1642, 'keywords')]
    },
    {
         'text': 'Computer vision is a rapidly growing field of artificial intelligence that enables computers to interpret and understand visual information from the world. By leveraging machine learning algorithms and large datasets, computer vision systems can detect and recognize objects, track movements, and analyze patterns in images and videos. This technology has numerous applications across various industries, including healthcare, security, retail, and manufacturing. For instance, computer vision can be used to develop autonomous vehicles that can detect pedestrians and obstacles, or to create medical imaging systems that can diagnose diseases more accurately. Additionally, computer vision can be used to improve quality control in manufacturing by detecting defects in products, or to enhance customer experience in retail by analyzing facial expressions and body language. As the field continues to evolve, we can expect to see even more innovative applications of computer vision in the years to come.',
         'entities': [(0, 15, 'keywords'), (46, 69, 'keywords'), (170, 197, 'keywords'), (517, 536, 'keywords'),
                      (714, 729, 'keywords')]
    },
    {
         'text': 'Long Short-Term Memory (LSTM) forecasting is a powerful technique used in time series forecasting to predict future values based on past data. LSTMs are a type of Recurrent Neural Network (RNN) that are particularly well-suited for modeling complex temporal relationships in data. By using LSTMs, forecasters can capture both short-term and long-term patterns in the data, allowing for more accurate predictions. In an LSTM forecasting model, the network learns to identify and remember important features in the data, such as trends, seasonality, and anomalies, and uses this information to generate predictions. This approach is particularly useful for forecasting tasks that involve non-linear relationships, such as stock prices, weather patterns, and traffic flow. By leveraging the capabilities of LSTMs, forecasters can create highly accurate models that can be used to inform business decisions, optimize operations, and mitigate risk.',
         'entities': [(0, 22, 'keywords'), (163, 187, 'keywords'), (655, 672, 'keywords'), (841, 856, 'keywords')]
    },
    {
         'text': 'Sentiment analysis is a subfield of natural language processing (NLP) that involves the use of algorithms and machine learning techniques to determine the emotional tone or attitude conveyed by a piece of text. This can include identifying whether the text is positive, negative, or neutral, as well as detecting the intensity of the sentiment and identifying the specific emotions or opinions expressed. Sentiment analysis has a wide range of applications, including customer service, market research, and social media monitoring. For example, companies can use sentiment analysis to analyze customer reviews and feedback to identify areas for improvement and to gauge customer satisfaction. Similarly, market researchers can use sentiment analysis to analyze social media posts and news articles to understand public opinion and sentiment towards a particular brand or product. By leveraging the power of sentiment analysis, businesses and organizations can gain valuable insights into the opinions and attitudes of their customers and stakeholders, and make data-driven decisions to improve their products and services.',
         'entities': [(0, 18, 'keywords'), (36, 63, 'keywords'), (110, 126, 'keywords')]
    },
    {
         'text': 'The distillation process in large language models is a technique used to improve the performance and efficiency of these models by transferring knowledge from a larger, more complex model to a smaller, simpler one. This process involves training a larger model, often referred to as the "teacher" model, on a large dataset and then using its output as the target for training a smaller model, referred to as the "student" model. The student model is trained to mimic the output of the teacher model, effectively learning the same patterns and relationships in the data. By doing so, the student model can achieve similar or even better performance than the teacher model, but with significantly fewer parameters and computational resources. This distillation process can be repeated multiple times, with the student model from one iteration serving as the teacher model for the next, allowing for further improvements in performance and efficiency. As a result, distillation has become a crucial component in the development of large language models, enabling the creation of smaller, more efficient models that can be deployed in a wide range of applications, from chatbots and virtual assistants to language translation and text summarization.',
         'entities': [(4, 16, 'keywords'), (28, 49, 'keywords'), (485, 498, 'keywords'), (587, 600, 'keywords'),
                      (1166, 1174, 'keywords'), (1179, 1197, 'keywords'), (1226, 1244, 'keywords')]
    },
    {
         'text': 'Self-supervised learning is a type of machine learning where a model is trained on a dataset without labeled examples, but instead learns to predict some aspect of the data itself. This approach has gained popularity in recent years due to its ability to leverage large amounts of unlabeled data, which is often readily available, and to learn features that are relevant to the task at hand. In self-supervised learning, the model is trained to predict a target output that is derived from the input data, such as the future state of a sequence, the similarity between two images, or the reconstruction of an input signal. By doing so, the model learns to extract meaningful representations from the data, which can be used for a wide range of tasks, including classification, regression, and clustering. Self-supervised learning has been shown to be particularly effective in areas such as computer vision, natural language processing, and speech recognition, where large amounts of unlabeled data are available. Additionally, self-supervised learning can be used to pre-train models, which can then be fine-tuned on a specific task, leading to improved performance and reduced overfitting.',
         'entities': [(0, 24, 'keywords'), (38, 54, 'keywords'), (891, 906, 'keywords'), (908, 935, 'keywords'),
                      (941, 959, 'keywords')]
    },
    {
         'text': "Quantization is a technique used to reduce the precision of a model's weights and activations from floating-point numbers to integers, typically 8-bit or 16-bit integers. This process is often used to reduce the memory footprint and computational requirements of a model, making it more suitable for deployment on resource-constrained devices such as mobile phones, embedded systems, and edge devices. Quantization works by approximating the floating-point values of the model's weights and activations with integer values that are close enough to the original values. This is typically done using techniques such as linear scaling, quantization-aware training, and knowledge distillation. Quantization can result in significant reductions in memory usage and computational requirements, while often maintaining a small loss in accuracy. For example, a model that requires 16 GB of memory and 100 GFLOPS of computation can be quantized to require only 1 GB of memory and 10 GFLOPS of computation, making it much more suitable for deployment on resource-constrained devices.",
         'entities': [(0, 12, 'keywords'), (201, 218, 'keywords'), (442, 456, 'keywords'), (666, 688, 'keywords'),
                      (760, 786, 'keywords')]
    },
    {
         'text': 'The Transformer is a type of neural network architecture that has revolutionized the field of natural language processing (NLP) and has been widely adopted in many applications, including machine translation, text summarization, and language modeling. Introduced in 2017, the Transformer architecture replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with self-attention mechanisms, which allow the model to attend to different parts of the input sequence simultaneously and weigh their importance. This allows the model to capture long-range dependencies and contextual relationships in the input data more effectively than traditional architectures. The Transformer architecture consists of an encoder and a decoder, with the encoder transforming the input sequence into a continuous representation and the decoder generating the output sequence. The Transformer has achieved state-of-the-art results in many NLP tasks and has become a fundamental building block in many NLP systems. Its success has also led to the development of many variants and extensions, including the BERT and RoBERTa models, which have further improved the state-of-the-art in many NLP tasks.',
         'entities': [(94, 121, 'keywords'), (322, 347, 'keywords'), (359, 388, 'keywords'), (401, 426, 'keywords'),
                      (701, 712, 'keywords'), (1122, 1126, 'keywords'), (1131, 1138, 'keywords')]
    },
    {
         'text': 'Group Query Attention is a type of attention mechanism that has been proposed to address the limitations of traditional attention mechanisms in handling long-range dependencies and complex relationships in data. In traditional attention mechanisms, a query is used to compute weights for different parts of the input sequence, but this can lead to issues such as vanishing gradients and difficulty in capturing long-range dependencies. GQA addresses these issues by introducing a group structure, where the input sequence is divided into smaller groups and a separate query is used for each group. This allows the model to focus on different parts of the input sequence simultaneously and capture complex relationships between groups. GQA has been shown to be effective in a variety of applications, including natural language processing, computer vision, and recommender systems. It has also been used to improve the performance of transformer-based models, which are known for their ability to capture long-range dependencies. By incorporating GQA into transformer-based models, researchers have been able to achieve state-of-the-art results in many NLP tasks, including machine translation, text classification, and question answering.',
         'entities': [(0, 21, 'keywords'), (120, 140, 'keywords'), (363, 382, 'keywords'), (810, 837, 'keywords'),
                      (839, 854, 'keywords')]
    },
    {
         'text': 'Machine learning is a subfield of artificial intelligence that involves the use of algorithms and statistical models to enable machines to learn from data and make predictions or decisions without being explicitly programmed. Machine learning involves training models on large datasets, where the model learns to identify patterns and relationships in the data, and uses this knowledge to make predictions or classify new, unseen data. Machine learning has many applications across various industries, including healthcare, finance, marketing, and more. For example, machine learning can be used to develop personalized medicine, predict customer behavior, and detect credit card fraud. Machine learning models can be trained using various techniques, including supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves training a model on labeled data, where the model learns to predict the correct output for a given input. Unsupervised learning involves training a model on unlabeled data, where the model learns to identify patterns and relationships in the data. Reinforcement learning involves training a model to make decisions by interacting with an environment and receiving rewards or penalties for its actions.',
         'entities': [(0, 16, 'keywords'), (34, 57, 'keywords'), (762, 781, 'keywords'), (783, 804, 'keywords'),
                      (810, 832, 'keywords')]
    },
    {
         'text': 'Image generation models are a type of artificial intelligence (AI) that use machine learning algorithms to generate new, original images from a given dataset or prompt. These models are trained on large collections of images and learn to recognize patterns, shapes, and textures, allowing them to create realistic and often photorealistic images. Image generation models can be used for a variety of applications, including generating new images for artistic purposes, creating synthetic data for training other AI models, and even generating images for use in advertising and marketing. Some popular types of image generation models include Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and StyleGANs. GANs consist of two neural networks that work together to generate new images, with one network generating images and the other network evaluating the generated images and providing feedback. VAEs use a neural network to learn a compressed representation of the input data and then generate new images by sampling from this representation. StyleGANs use a neural network to learn the style of an image and then generate new images by applying this style to a random noise vector.',
         'entities': [(0, 16, 'keywords'), (38, 61, 'keywords'), (642, 673, 'keywords'), (682, 706, 'keywords'),
                      (719, 728, 'keywords'), (933, 947, 'keywords')]
    },
    {
         'text': 'Further complicating the development of new foundation models for robotics\nis ensuring their safety and robustness when training or deploying them in the real world. We\ncan expect the safety risks from these models for robotics to be different from their language\ncounterparts given that embodied agents are empowered to manipulate and interact with their\nsurroundings directly in the physical world. One core safety challenge for learning-based systems is\nthe chicken-and-egg problem of needing to specify system constraints for safety prior to collecting\ndata, after which unforeseen unsafe behaviors requiring additional constraints may emerge. For\ninstance, an agent adapting to a new kitchen outside of the training distribution requires sufficient\nsafety guarantees to ensure safe data collection, which may either adversely affect task performance\nor cause the agent to fail in novel ways. One way to resolve this is restricting the complexity of\nthe environment or increasing the complexity of the robot such that irrecoverable states or unsafe\nactions are avoided by construction.',
         'entities': [(44, 61, 'keywords'), (66, 74, 'keywords'), (754, 771, 'keywords')]
    },
    {
         'text': 'The Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream\ntasks with different data modalities. A pretrained foundation model, such as BERT, GPT-3, MAE,\nDALLE-E, and ChatGPT, is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. The idea of pretraining behind PFMs plays an\nimportant role in the application of large models. As a type of transfer learning paradigm, pretraining\nis applied in computer vision with frozen and fine-tuning techniques, showing promising performance.\nWord embedding in the natural language process can be also regarded as a type of pertaining, but it\nsuffers many problems such as polysemy. Different from previous methods that apply convolution and\nrecurrent modules for feature extractions, the generative pre-training (GPT) method applies Transformer\nas the feature extractor and is trained on large datasets with an autoregressive paradigm. Similarly,\nthe BERT apples transformers to train on large datasets as a contextual language model. Recently, the\nChatGPT shows promising success on large language models, which applies an autoregressive language\nmodel with zero shot or few show prompting. With the extraordinary success of PFMs, AI has made\nwaves in a variety of fields over the past few years. Considerable methods, datasets, and evaluation\nmetrics have been proposed in the literature, the need is raising for an updated survey.\nThis study provides a comprehensive review of recent research advancements, current and future\nchallenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. We first\nreview the basic components and existing pretraining in natural language processing, computer vision,\nand graph learning. We then discuss other advanced PFMs for other data modalities and unified PFMs\nconsidering the data quality and quantity. Besides, we discuss relevant research about the fundamentals\nof the PFM, including model efficiency and compression, security, and privacy. Finally, we lay out key\nimplications, future research directions, challenges, and open problems. We hope this survey can shed\nlight on the research of PFMs on the scalability, reasoning ability, cross-domain ability, user',
         'entities': [(4, 32, 'keywords'), (171, 175, 'keywords'), (177, 182, 'keywords'), (184, 187, 'keywords'),
                      (189, 196, 'keywords'), (202, 209, 'keywords'), (315, 338, 'keywords'), (503, 518, 'keywords'),
                      (881, 892, 'keywords'), (1738, 1765, 'keywords')]
    },
    {
         'text': 'Self-supervised learning has been shown to be very effective in learning useful\nrepresentations, and yet much of the success is achieved in data types such as\nimages, audio, and text. The success is mainly enabled by taking advantage of\nspatial, temporal, or semantic structure in the data through augmentation. However,\nsuch structure may not exist in tabular datasets commonly used in fields such as\nhealthcare, making it difficult to design an effective augmentation method, and\nhindering a similar progress in tabular data setting. In this paper, we introduce a\nnew framework, Subsetting features of Tabular data (SubTab), that turns the task\nof learning from tabular data into a multi-view representation learning problem by\ndividing the input features to multiple subsets. We argue that reconstructing the data\nfrom the subset of its features rather than its corrupted version in an autoencoder\nsetting can better capture its underlying latent representation. In this framework,\nthe joint representation can be expressed as the aggregate of latent variables of the\nsubsets at test time, which we refer to as collaborative inference. Our experiments\nshow that the SubTab achieves the state of the art (SOTA) performance of 98.31%\non MNIST in tabular setting, on par with CNN-based SOTA models, and surpasses\nexisting baselines on three other real-world datasets by a significant margin.',
         'entities': [(0, 24, 'keywords'), (298, 310, 'keywords'), (604, 616, 'keywords'), (889, 900, 'keywords')]}, {
         'text': 'We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet\nuses sequential attention to choose which features to reason\nfrom at each decision step, enabling interpretability and more\nefficient learning as the learning capacity is used for the most\nsalient features. We demonstrate that TabNet outperforms\nother variants on a wide range of non-performance-saturated\ntabular datasets and yields interpretable feature attributions\nplus insights into its global behavior. Finally, we demonstrate\nself-supervised learning for tabular data, significantly improving performance when unlabeled data is abundant.',
         'entities': [(105, 111, 'keywords'), (125, 145, 'keywords'), (553, 577, 'keywords'), (582, 594, 'keywords')]
    },
    {
         'text': 'Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge\ncontained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced\nfor constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in\nmachine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce\napproaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer\nlearning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect\nand systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of\ntransfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and\nideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially\nhomogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also\nbriefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning\nmodels are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and\nOffice-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different\napplications in practice.',
         'entities': [(74, 88, 'keywords'), (109, 118, 'keywords'), (326, 343, 'keywords'), (387, 403, 'keywords')]
    },
    {
         'text': 'Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level, left-to-right\ndecision-making processes during inference. This means they can fall short in\ntasks that require exploration, strategic lookahead, or where initial decisions play\na pivotal role. To surmount these challenges, we introduce a new framework for\nlanguage model inference, “Tree of Thoughts” (ToT), which generalizes over the\npopular “Chain of Thought” approach to prompting language models, and enables\nexploration over coherent units of text (“thoughts”) that serve as intermediate steps\ntoward problem solving. ToT allows LMs to perform deliberate decision making\nby considering multiple different reasoning paths and self-evaluating choices to\ndecide the next course of action, as well as looking ahead or backtracking when\nnecessary to make global choices. Our experiments show that ToT significantly\nenhances language models’ problem-solving abilities on three novel tasks requiring\nnon-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords.\nFor instance, in Game of 24, while GPT-4 with chain-of-thought prompting only\nsolved 4% of tasks, our method achieved a success rate of 74%. Code repo with all\nprompts.',
         'entities': [(428, 444, 'keywords'), (489, 505, 'keywords'), (529, 544, 'keywords'),
                      (1161, 1166, 'keywords')]
    },
    {
         'text': 'Natural language understanding comprises a wide range of diverse tasks such\nas textual entailment, question answering, semantic similarity assessment, and\ndocument classification. Although large unlabeled text corpora are abundant,\nlabeled data for learning these specific tasks is scarce, making it challenging for\ndiscriminatively trained models to perform adequately. We demonstrate that large\ngains on these tasks can be realized by generative pre-training of a language model\non a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each\nspecific task. In contrast to previous approaches, we make use of task-aware input\ntransformations during fine-tuning to achieve effective transfer while requiring\nminimal changes to the model architecture. We demonstrate the effectiveness of\nour approach on a wide range of benchmarks for natural language understanding.\nOur general task-agnostic model outperforms discriminatively trained models that\nuse architectures specifically crafted for each task, significantly improving upon the\nstate of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute\nimprovements of 8.9% on commonsense reasoning, 5.7% on\nquestion answering (RACE), and 1.5% on textual entailment (MultiNLI).',
         'entities': [(79, 97, 'keywords'), (119, 138, 'keywords'), (857, 873, 'keywords'), (1197, 1215, 'keywords')]
    },
    {
         'text': 'We evaluate our approach on four types of language understanding tasks natural language inference,\nquestion answering, semantic similarity, and text classification. Our general task-agnostic model\noutperforms discriminatively trained models that employ architectures specifically crafted for each\ntask, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance,\nwe achieve absolute improvements of 8.9% on commonsense reasoning,\n5.7% on question answering (RACE) [30], 1.5% on textual entailment and 5.5% on\nthe recently introduced GLUE multi-task benchmark. We also analyzed zero-shot behaviors\nof the pre-trained model on four different settings and demonstrate that it acquires useful linguistic\nknowledge for downstream tasks.',
         'entities': [(119, 138, 'keywords'), (144, 163, 'keywords'), (571, 575, 'keywords'), (615, 624, 'keywords'),
                      (752, 768, 'keywords')]
    },
    {
         'text': 'The closest line of work to ours involves pre-training a neural network using a language modeling\nobjective and then fine-tuning it on a target task with supervision and Howard and\nRuder follow this method to improve text classification. However, although the pre-training\nphase helps capture some linguistic information, their usage of LSTM models restricts their prediction\nability to a short range. In contrast, our choice of transformer networks allows us to capture longerrange linguistic structure, as demonstrated in our experiments. Further, we also demonstrate the\neffectiveness of our model on a wider range of tasks including natural language inference, paraphrase\ndetection and story completion.',
         'entities': [(57, 71, 'keywords'), (80, 97, 'keywords'), (337, 341, 'keywords'), (429, 440, 'keywords')]
    },
    {
         'text': 'For some tasks, like text classification, we can directly fine-tune our model as described above.\nCertain other tasks, like question answering or textual entailment, have structured inputs such as\nordered sentence pairs, or triplets of document, question, and answers. Since our pre-trained model\nwas trained on contiguous sequences of text, we require some modifications to apply it to these tasks.\nPrevious work proposed learning task specific architectures on top of transferred representations.\nSuch an approach re-introduces a significant amount of task-specific customization and does not\nuse transfer learning for these additional architectural components. Instead, we use a traversal-style\napproach, where we convert structured inputs into an ordered sequence that our pre-trained\nmodel can process. These input transformations allow us to avoid making extensive changes to the\narchitecture across tasks. We provide a brief description of these input transformations below and\nFigure 1 provides a visual illustration.',
         'entities': [(21, 40, 'keywords'), (146, 164, 'keywords'), (279, 296, 'keywords'), (599, 616, 'keywords')]
    }
]