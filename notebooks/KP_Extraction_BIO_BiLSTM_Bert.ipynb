{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "sample_data = [\n",
        "    (\"Deep learning models have revolutionized computer vision tasks.\",\n",
        "     \"B-KP I-KP I-KP O O B-KP I-KP O\"),\n",
        "    (\"Neural networks process data efficiently.\",\n",
        "     \"B-KP I-KP O B-KP O\"),\n",
        "    (\"Machine learning algorithms improve performance metrics.\",\n",
        "     \"B-KP I-KP I-KP O B-KP I-KP\"),\n",
        "    (\"The transformer architecture enables efficient processing.\",\n",
        "     \"O B-KP I-KP O O O\"),\n",
        "    (\"Data science techniques analyze large datasets.\",\n",
        "     \"B-KP I-KP I-KP O O O\")\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "Gf17zmMBKS6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiLSTM"
      ],
      "metadata": {
        "id": "e8QL-HITKBCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import spacy\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Example data with BIO tags\n",
        "texts = [\n",
        "    \"Deep learning is revolutionizing artificial intelligence research\",\n",
        "    \"Climate change poses significant environmental challenges\",\n",
        "    \"Quantum computing could transform cryptography\",\n",
        "    \"Renewable energy sources reduce carbon emissions\",\n",
        "    \"Machine learning algorithms improve decision making\",\n",
        "    \"Space exploration advances technological innovation\",\n",
        "    \"Biotechnology developments impact medical research\",\n",
        "    \"Digital transformation reshapes business operations\",\n",
        "    \"Neural networks enable pattern recognition\",\n",
        "    \"Sustainable development promotes environmental conservation\"\n",
        "]\n",
        "\n",
        "# BIO tags for each token in the texts\n",
        "# B: Beginning of keyphrase (B-KP)\n",
        "# I: Inside of keyphrase (I-KP)\n",
        "# O: Outside of keyphrase (O)\n",
        "bio_tags = [\n",
        "    [\"B\", \"I\", \"O\", \"O\", \"B\", \"I\", \"O\"],  # Deep learning, artificial intelligence\n",
        "    [\"B\", \"I\", \"O\", \"O\", \"B\", \"I\"],  # Climate change, environmental challenges\n",
        "    [\"B\", \"I\", \"O\", \"O\", \"B\"],  # Quantum computing, cryptography\n",
        "    [\"B\", \"I\", \"O\", \"O\", \"B\", \"I\"],  # Renewable energy, carbon emissions\n",
        "    [\"B\", \"I\", \"O\", \"O\", \"O\", \"I\"],  # Machine learning, decision making\n",
        "    [\"B\", \"I\", \"O\", \"B\", \"I\"],  # Space exploration, technological innovation\n",
        "    [\"B\", \"O\", \"O\", \"B\", \"I\"],  # Biotechnology, medical research\n",
        "    [\"B\", \"I\", \"O\", \"B\", \"I\"],  # Digital transformation, business operations\n",
        "    [\"B\", \"I\", \"O\", \"B\", \"I\"],  # Neural networks, pattern recognition\n",
        "    [\"B\", \"I\", \"O\", \"B\", \"I\"]   # Sustainable development, environmental conservation\n",
        "]\n",
        "\n",
        "# Create vocabularies\n",
        "def build_vocab(texts):\n",
        "    word_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "    for text in texts:\n",
        "        doc = nlp(text.lower())\n",
        "        for token in doc:\n",
        "            if token.text not in word_vocab:\n",
        "                word_vocab[token.text] = len(word_vocab)\n",
        "    return word_vocab\n",
        "\n",
        "def build_tag_vocab():\n",
        "    return {'B': 0, 'I': 1, 'O': 2, '<PAD>': 3}\n",
        "\n",
        "word_vocab = build_vocab(texts)\n",
        "tag_vocab = build_tag_vocab()\n",
        "tag_vocab_reverse = {v: k for k, v in tag_vocab.items()}\n",
        "\n",
        "# Dataset class\n",
        "class KeyPhraseDataset(Dataset):\n",
        "    def __init__(self, texts, bio_tags, word_vocab, tag_vocab):\n",
        "        self.texts = texts\n",
        "        self.bio_tags = bio_tags\n",
        "        self.word_vocab = word_vocab\n",
        "        self.tag_vocab = tag_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        doc = nlp(text.lower())\n",
        "\n",
        "        # Convert text to indices\n",
        "        indices = [self.word_vocab.get(token.text, self.word_vocab['<UNK>'])\n",
        "                  for token in doc]\n",
        "\n",
        "        # Convert BIO tags to indices\n",
        "        tag_indices = [self.tag_vocab[tag] for tag in self.bio_tags[idx]]\n",
        "\n",
        "        return (torch.tensor(indices),\n",
        "                torch.tensor(tag_indices),\n",
        "                len(indices))\n",
        "\n",
        "# BiLSTM model\n",
        "class KeyPhraseBiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags):\n",
        "        super(KeyPhraseBiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                           bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_tags)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed = pack_padded_sequence(embedded, lengths,\n",
        "                                    batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        lstm_out, _ = self.lstm(packed)\n",
        "\n",
        "        # Unpack sequence\n",
        "        unpacked, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        # Linear layer\n",
        "        logits = self.fc(unpacked)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    texts, tags, lengths = zip(*batch)\n",
        "\n",
        "    # Pad sequences\n",
        "    texts_padded = pad_sequence(texts, batch_first=True)\n",
        "    tags_padded = pad_sequence(tags, batch_first=True,\n",
        "                              padding_value=tag_vocab['<PAD>'])\n",
        "\n",
        "    return texts_padded, tags_padded, torch.tensor(lengths)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = KeyPhraseDataset(texts, bio_tags, word_vocab, tag_vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = KeyPhraseBiLSTM(len(word_vocab), embedding_dim=100,\n",
        "                        hidden_dim=64, num_tags=len(tag_vocab))\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tag_vocab['<PAD>'])\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_texts, batch_tags, batch_lengths in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(batch_texts, batch_lengths)\n",
        "\n",
        "        # Reshape for loss calculation\n",
        "        logits = logits.view(-1, len(tag_vocab))\n",
        "        batch_tags = batch_tags.view(-1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(logits, batch_tags)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# Example inference\n",
        "def predict_keyphrases(text, model, word_vocab, tag_vocab_reverse):\n",
        "    model.eval()\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.text for token in doc]\n",
        "    indices = [word_vocab.get(token, word_vocab['<UNK>']) for token in tokens]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor(indices).unsqueeze(0)\n",
        "        lengths = torch.tensor([len(indices)])\n",
        "        logits = model(input_tensor, lengths)\n",
        "        predictions = torch.argmax(logits, dim=-1)[0]\n",
        "\n",
        "    # Convert predictions to BIO tags\n",
        "    predicted_tags = [tag_vocab_reverse[pred.item()] for pred in predictions]\n",
        "\n",
        "    # Extract keyphrases based on BIO tags\n",
        "    keyphrases = []\n",
        "    current_phrase = []\n",
        "\n",
        "    for token, tag in zip(tokens, predicted_tags):\n",
        "        if tag == 'B':\n",
        "            if current_phrase:\n",
        "                keyphrases.append(\" \".join(current_phrase))\n",
        "            current_phrase = [token]\n",
        "        elif tag == 'I' and current_phrase:\n",
        "            current_phrase.append(token)\n",
        "        elif tag == 'O' and current_phrase:\n",
        "            keyphrases.append(\" \".join(current_phrase))\n",
        "            current_phrase = []\n",
        "\n",
        "    if current_phrase:\n",
        "        keyphrases.append(\" \".join(current_phrase))\n",
        "\n",
        "    return keyphrases\n",
        "\n",
        "# Test prediction\n",
        "test_text = \"what is the purpose of CNN in using deep learning\"\n",
        "predicted_keyphrases = predict_keyphrases(test_text, model, word_vocab, tag_vocab_reverse)\n",
        "print(f\"\\nTest text: {test_text}\")\n",
        "print(f\"Predicted keyphrases: {predicted_keyphrases}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB7ZtvrvGEAQ",
        "outputId": "25306ee9-3045-4621-f8f8-9388fc6f86cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.3739\n",
            "Epoch 2/10, Loss: 1.2788\n",
            "Epoch 3/10, Loss: 1.1981\n",
            "Epoch 4/10, Loss: 1.1170\n",
            "Epoch 5/10, Loss: 1.0408\n",
            "Epoch 6/10, Loss: 0.9510\n",
            "Epoch 7/10, Loss: 0.8633\n",
            "Epoch 8/10, Loss: 0.7726\n",
            "Epoch 9/10, Loss: 0.6741\n",
            "Epoch 10/10, Loss: 0.5776\n",
            "\n",
            "Test text: what is the purpose of CNN in using deep learning\n",
            "Predicted keyphrases: ['what', 'the', 'purpose', 'of', 'cnn', 'in', 'using', 'deep learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zM3qfH3GEHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# bert-base-uncased"
      ],
      "metadata": {
        "id": "AQovxXLgJ3Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example data with BIO tags\n",
        "texts = [\n",
        "    \"Deep learning is revolutionizing artificial intelligence research\",\n",
        "    \"Climate change poses significant environmental challenges\",\n",
        "    \"Quantum computing could transform cryptography\",\n",
        "    \"Renewable energy sources reduce carbon emissions\",\n",
        "    \"Machine learning algorithms improve decision making\",\n",
        "    \"Space exploration advances technological innovation\",\n",
        "    \"Biotechnology developments impact medical research\",\n",
        "    \"Digital transformation reshapes business operations\",\n",
        "    \"Neural networks enable pattern recognition\",\n",
        "    \"Sustainable development promotes environmental conservation\"\n",
        "]\n",
        "\n",
        "# BIO tags for each token in the texts\n",
        "bio_tags = [\n",
        "    [\"B\", \"I\", \"O\", \"O\", \"B\", \"I\", \"O\"],  # Deep learning, artificial intelligence\n",
        "    [\"B\", \"I\", \"O\", \"O\", \"B\", \"I\"],  # Climate change, environmental challenges\n",
        "    [\"B\", \"I\", \"O\", \"O\", \"B\"],  # Quantum computing, cryptography\n",
        "    [\"B\", \"I\", \"O\", \"O\", \"B\", \"I\"],  # Renewable energy, carbon emissions\n",
        "    [\"B\", \"I\", \"O\", \"O\", \"O\", \"I\"],  # Machine learning, decision making\n",
        "    [\"B\", \"I\", \"O\", \"B\", \"I\"],  # Space exploration, technological innovation\n",
        "    [\"B\", \"O\", \"O\", \"B\", \"I\"],  # Biotechnology, medical research\n",
        "    [\"B\", \"I\", \"O\", \"B\", \"I\"],  # Digital transformation, business operations\n",
        "    [\"B\", \"I\", \"O\", \"B\", \"I\"],  # Neural networks, pattern recognition\n",
        "    [\"B\", \"I\", \"O\", \"B\", \"I\"]   # Sustainable development, environmental conservation\n",
        "]\n",
        "\n",
        "# Create tag vocabulary\n",
        "tag2idx = {'B': 0, 'I': 1, 'O': 2, 'PAD': 3}\n",
        "idx2tag = {v: k for k, v in tag2idx.items()}\n",
        "\n",
        "class KeyPhraseDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], bio_tags: List[List[str]], tokenizer, tag2idx: dict):\n",
        "        self.texts = texts\n",
        "        self.bio_tags = bio_tags\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tag2idx = tag2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        tags = self.bio_tags[idx]\n",
        "\n",
        "        # Tokenize text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Get input_ids and attention_mask\n",
        "        input_ids = encoding['input_ids'].squeeze(0)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Create token type IDs\n",
        "        token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # Align BIO tags with BERT tokens\n",
        "        bert_tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
        "        aligned_labels = self.align_labels_with_tokens(tags, bert_tokens)\n",
        "\n",
        "        # Convert tags to tensor\n",
        "        label_ids = torch.tensor([self.tag2idx.get(label, self.tag2idx['O'])\n",
        "                                for label in aligned_labels])\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'token_type_ids': token_type_ids,\n",
        "            'labels': label_ids\n",
        "        }\n",
        "\n",
        "    def align_labels_with_tokens(self, tags: List[str], bert_tokens: List[str]) -> List[str]:\n",
        "        aligned_labels = ['O']  # for [CLS]\n",
        "        orig_token_idx = 0\n",
        "\n",
        "        for bert_token in bert_tokens[1:-1]:  # Skip [CLS] and [SEP]\n",
        "            if bert_token.startswith('##'):\n",
        "                if orig_token_idx > 0:\n",
        "                    aligned_labels.append(tags[orig_token_idx-1])\n",
        "            else:\n",
        "                if orig_token_idx < len(tags):\n",
        "                    aligned_labels.append(tags[orig_token_idx])\n",
        "                    orig_token_idx += 1\n",
        "                else:\n",
        "                    aligned_labels.append('O')\n",
        "\n",
        "        aligned_labels.append('O')  # for [SEP]\n",
        "        return aligned_labels\n",
        "\n",
        "class BERTForKeyPhraseExtraction(nn.Module):\n",
        "    def __init__(self, num_labels: int):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_len = max([len(item['input_ids']) for item in batch])\n",
        "\n",
        "    input_ids = torch.stack([\n",
        "        torch.nn.functional.pad(item['input_ids'], (0, max_len - len(item['input_ids'])), value=tokenizer.pad_token_id)\n",
        "        for item in batch\n",
        "    ])\n",
        "\n",
        "    attention_mask = torch.stack([\n",
        "        torch.nn.functional.pad(item['attention_mask'], (0, max_len - len(item['attention_mask'])), value=0)\n",
        "        for item in batch\n",
        "    ])\n",
        "\n",
        "    token_type_ids = torch.stack([\n",
        "        torch.nn.functional.pad(item['token_type_ids'], (0, max_len - len(item['token_type_ids'])), value=0)\n",
        "        for item in batch\n",
        "    ])\n",
        "\n",
        "    labels = torch.stack([\n",
        "        torch.nn.functional.pad(item['labels'], (0, max_len - len(item['labels'])), value=tag2idx['PAD'])\n",
        "        for item in batch\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = KeyPhraseDataset(texts, bio_tags, tokenizer, tag2idx)\n",
        "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BERTForKeyPhraseExtraction(num_labels=len(tag2idx))\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx['PAD'])\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        token_type_ids = batch['token_type_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(logits.view(-1, len(tag2idx)), labels.view(-1))\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0UUbwtFKUZY",
        "outputId": "9b966a83-1a8d-4a1d-9471-78ab24074e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Loss: 1.2703\n",
            "Epoch 2/25, Loss: 0.8663\n",
            "Epoch 3/25, Loss: 0.6358\n",
            "Epoch 4/25, Loss: 0.4443\n",
            "Epoch 5/25, Loss: 0.3155\n",
            "Epoch 6/25, Loss: 0.1900\n",
            "Epoch 7/25, Loss: 0.1217\n",
            "Epoch 8/25, Loss: 0.0868\n",
            "Epoch 9/25, Loss: 0.0534\n",
            "Epoch 10/25, Loss: 0.0414\n",
            "Epoch 11/25, Loss: 0.0302\n",
            "Epoch 12/25, Loss: 0.0239\n",
            "Epoch 13/25, Loss: 0.0172\n",
            "Epoch 14/25, Loss: 0.0132\n",
            "Epoch 15/25, Loss: 0.0128\n",
            "Epoch 16/25, Loss: 0.0093\n",
            "Epoch 17/25, Loss: 0.0085\n",
            "Epoch 18/25, Loss: 0.0074\n",
            "Epoch 19/25, Loss: 0.0077\n",
            "Epoch 20/25, Loss: 0.0067\n",
            "Epoch 21/25, Loss: 0.0057\n",
            "Epoch 22/25, Loss: 0.0053\n",
            "Epoch 23/25, Loss: 0.0054\n",
            "Epoch 24/25, Loss: 0.0052\n",
            "Epoch 25/25, Loss: 0.0044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_keyphrases(text: str, model, tokenizer, tag2idx, idx2tag):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize input text\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    token_type_ids = torch.zeros_like(input_ids).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask, token_type_ids)\n",
        "        predictions = torch.argmax(logits, dim=-1)[0]\n",
        "\n",
        "    # Convert predictions to tags\n",
        "    predicted_tags = [idx2tag[pred.item()] for pred in predictions]\n",
        "\n",
        "    # Get original tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "    # Extract keyphrases\n",
        "    keyphrases = []\n",
        "    current_phrase = []\n",
        "\n",
        "    for token, tag in zip(tokens[1:-1], predicted_tags[1:-1]):  # Skip [CLS] and [SEP]\n",
        "        if tag == 'B':\n",
        "            if current_phrase:\n",
        "                keyphrases.append(\" \".join(current_phrase))\n",
        "            current_phrase = [token.replace('##', '')]\n",
        "        elif tag == 'I' and current_phrase:\n",
        "            current_phrase.append(token.replace('##', ''))\n",
        "        elif tag == 'O' and current_phrase:\n",
        "            keyphrases.append(\" \".join(current_phrase))\n",
        "            current_phrase = []\n",
        "\n",
        "    if current_phrase:\n",
        "        keyphrases.append(\" \".join(current_phrase))\n",
        "\n",
        "    return keyphrases\n",
        "\n",
        "# Test prediction\n",
        "test_text = \"Artificial intelligence applications transform modern technology\"\n",
        "predicted_keyphrases = predict_keyphrases(test_text, model, tokenizer, tag2idx, idx2tag)\n",
        "print(f\"\\nTest text: {test_text}\")\n",
        "print(f\"Predicted keyphrases: {predicted_keyphrases}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ5s65IkKUcV",
        "outputId": "b47d926a-9cc4-46d2-c7f0-7d3a1aaf666e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test text: Artificial intelligence applications transform modern technology\n",
            "Predicted keyphrases: ['artificial intelligence', 'modern technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "iiY6r4qJbWPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"what is the purpose of CNN in using deep learning\"\n",
        "predicted_keyphrases = predict_keyphrases(test_text, model, tokenizer, tag2idx, idx2tag)\n",
        "print(f\"\\nTest text: {test_text}\")\n",
        "print(f\"Predicted keyphrases: {predicted_keyphrases}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLnP7KKjKUfF",
        "outputId": "6192f4a5-889b-47ec-b25a-83ea4ce89c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test text: what is the purpose of CNN in using deep learning\n",
            "Predicted keyphrases: ['cnn', 'deep learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"I have used large language model for the question answering task\"\n",
        "predicted_keyphrases = predict_keyphrases(test_text, model, tokenizer, tag2idx, idx2tag)\n",
        "print(f\"\\nTest text: {test_text}\")\n",
        "print(f\"Predicted keyphrases: {predicted_keyphrases}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io8f_sgFKUil",
        "outputId": "ffedc73c-8c4e-4c23-a1ab-f03d269b201f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test text: I have used large language model for the question answering task\n",
            "Predicted keyphrases: ['large', 'language model', 'question answering']\n"
          ]
        }
      ]
    }
  ]
}