{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Extended NER dataset with more examples and diverse entities\n",
        "train_data = [\n",
        "    # Original examples\n",
        "    {\n",
        "        \"text\": \"John works at Google in New York\",\n",
        "        \"entities\": [(0, 4, \"PER\"), (14, 20, \"ORG\"), (24, 32, \"LOC\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Microsoft announced new AI features yesterday\",\n",
        "        \"entities\": [(0, 9, \"ORG\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Sarah visited Paris last summer\",\n",
        "        \"entities\": [(0, 5, \"PER\"), (13, 18, \"LOC\")]\n",
        "    },\n",
        "\n",
        "    # Tech domain examples\n",
        "    {\n",
        "        \"text\": \"Sundar Pichai leads Alphabet and Google in Silicon Valley\",\n",
        "        \"entities\": [(0, 13, \"PER\"), (20, 28, \"ORG\"), (33, 39, \"ORG\"), (43, 56, \"LOC\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Tesla's Elon Musk announced plans to expand operations in Shanghai\",\n",
        "        \"entities\": [(0, 5, \"ORG\"), (7, 16, \"PER\"), (49, 57, \"LOC\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Amazon Web Services opened new data centers in Mumbai\",\n",
        "        \"entities\": [(0, 19, \"ORG\"), (47, 53, \"LOC\")]\n",
        "    },\n",
        "\n",
        "    # Business/Finance examples\n",
        "    {\n",
        "        \"text\": \"JPMorgan Chase CEO Jamie Dimon spoke at the World Economic Forum in Davos\",\n",
        "        \"entities\": [(0, 14, \"ORG\"), (19, 30, \"PER\"), (42, 62, \"ORG\"), (66, 71, \"LOC\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Goldman Sachs opened its new office in London last month\",\n",
        "        \"entities\": [(0, 13, \"ORG\"), (37, 43, \"LOC\")]\n",
        "    },\n",
        "\n",
        "    # Sports examples\n",
        "    {\n",
        "        \"text\": \"Lionel Messi scored two goals for Inter Miami at DRV PNK Stadium\",\n",
        "        \"entities\": [(0, 12, \"PER\"), (31, 42, \"ORG\"), (46, 61, \"LOC\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Manchester United defeated Arsenal at Emirates Stadium\",\n",
        "        \"entities\": [(0, 16, \"ORG\"), (25, 32, \"ORG\"), (36, 52, \"LOC\")]\n",
        "    },\n",
        "\n",
        "    # Entertainment examples\n",
        "    {\n",
        "        \"text\": \"Tom Cruise attended the premiere in Hollywood\",\n",
        "        \"entities\": [(0, 10, \"PER\"), (35, 44, \"LOC\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Netflix released a new series produced by Warner Bros in Los Angeles\",\n",
        "        \"entities\": [(0, 7, \"ORG\"), (39, 50, \"ORG\"), (54, 65, \"LOC\")]\n",
        "    },\n",
        "\n",
        "    # Political examples\n",
        "    {\n",
        "        \"text\": \"President Biden met with Chancellor Scholz at the White House\",\n",
        "        \"entities\": [(10, 15, \"PER\"), (31, 37, \"PER\"), (45, 56, \"LOC\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"The United Nations headquarters in New York hosted the summit\",\n",
        "        \"entities\": [(4, 18, \"ORG\"), (34, 42, \"LOC\")]\n",
        "    },\n",
        "\n",
        "    # Academic/Research examples\n",
        "    {\n",
        "        \"text\": \"Dr. Smith from Stanford University published groundbreaking research at MIT\",\n",
        "        \"entities\": [(4, 9, \"PER\"), (15, 33, \"ORG\"), (63, 66, \"ORG\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Harvard researchers collaborated with Oxford University on AI development\",\n",
        "        \"entities\": [(0, 7, \"ORG\"), (36, 52, \"ORG\")]\n",
        "    },\n",
        "\n",
        "    # Mixed complex examples\n",
        "    {\n",
        "        \"text\": \"While Mark Zuckerberg was presenting at Facebook, Tim Cook announced Apple's latest iPhone in California\",\n",
        "        \"entities\": [(6, 21, \"PER\"), (39, 47, \"ORG\"), (49, 57, \"PER\"), (68, 73, \"ORG\"), (91, 101, \"LOC\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"The BBC reported that Samsung's CEO visited their facilities in Seoul and Tokyo\",\n",
        "        \"entities\": [(4, 7, \"ORG\"), (21, 28, \"ORG\"), (51, 56, \"LOC\"), (61, 66, \"LOC\")]\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Deutsche Bank's Frankfurt office contacted Morgan Stanley in London regarding the merger\",\n",
        "        \"entities\": [(0, 13, \"ORG\"), (15, 24, \"LOC\"), (35, 48, \"ORG\"), (52, 58, \"LOC\")]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Define label to ID mapping\n",
        "label2id = {\n",
        "    \"O\": 0,\n",
        "    \"B-PER\": 1,\n",
        "    \"I-PER\": 2,\n",
        "    \"B-ORG\": 3,\n",
        "    \"I-ORG\": 4,\n",
        "    \"B-LOC\": 5,\n",
        "    \"I-LOC\": 6\n",
        "}\n",
        "\n",
        "id2label = {v: k for k, v in label2id.items()}"
      ],
      "metadata": {
        "id": "z2Mr4HRtVcHn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertForTokenClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jKS-koB3Vi5a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"text\"]\n",
        "        entities = item[\"entities\"]\n",
        "\n",
        "        # Create label sequence\n",
        "        labels = [\"O\"] * len(text)\n",
        "        for start, end, label in entities:\n",
        "            labels[start] = f\"B-{label}\"\n",
        "            for i in range(start + 1, end):\n",
        "                labels[i] = f\"I-{label}\"\n",
        "\n",
        "        # Tokenize text and align labels\n",
        "        tokenized = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_offsets_mapping=True\n",
        "        )\n",
        "\n",
        "        aligned_labels = [-100] * len(tokenized[\"input_ids\"])  # -100 is ignored in loss calculation\n",
        "        offset_mapping = tokenized[\"offset_mapping\"]\n",
        "\n",
        "        for idx, (start, end) in enumerate(offset_mapping):\n",
        "            if start == end:  # Special tokens\n",
        "                continue\n",
        "            # Find the label for this token\n",
        "            token_label = labels[start]\n",
        "            aligned_labels[idx] = label2id[token_label]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(tokenized[\"input_ids\"]),\n",
        "            \"attention_mask\": torch.tensor(tokenized[\"attention_mask\"]),\n",
        "            \"labels\": torch.tensor(aligned_labels)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "dFkuRgTlVpLZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, num_epochs=3, device=\"cuda\"):\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "oBTwTWzyWAyX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxsw2SdvT1E-",
        "outputId": "e4cc8778-0914-40ff-bf46-eb3ff0075291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.eos_token = tokenizer.pad_token\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and dataloader\n",
        "dataset = NERDataset(train_data, tokenizer)\n",
        "train_dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "IXoQRji2Whdn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_model(model, train_dataloader, num_epochs=25, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5nVwzpzWkxq",
        "outputId": "f36aea5a-3787-4079-b6bc-2db2e8dc1f16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Average Loss: 1.8040\n",
            "Epoch 2/25, Average Loss: 1.2346\n",
            "Epoch 3/25, Average Loss: 0.9982\n",
            "Epoch 4/25, Average Loss: 0.7811\n",
            "Epoch 5/25, Average Loss: 0.6455\n",
            "Epoch 6/25, Average Loss: 0.5155\n",
            "Epoch 7/25, Average Loss: 0.4112\n",
            "Epoch 8/25, Average Loss: 0.3495\n",
            "Epoch 9/25, Average Loss: 0.2419\n",
            "Epoch 10/25, Average Loss: 0.2102\n",
            "Epoch 11/25, Average Loss: 0.1690\n",
            "Epoch 12/25, Average Loss: 0.1249\n",
            "Epoch 13/25, Average Loss: 0.1352\n",
            "Epoch 14/25, Average Loss: 0.0793\n",
            "Epoch 15/25, Average Loss: 0.0729\n",
            "Epoch 16/25, Average Loss: 0.0570\n",
            "Epoch 17/25, Average Loss: 0.0448\n",
            "Epoch 18/25, Average Loss: 0.0334\n",
            "Epoch 19/25, Average Loss: 0.0276\n",
            "Epoch 20/25, Average Loss: 0.0282\n",
            "Epoch 21/25, Average Loss: 0.0235\n",
            "Epoch 22/25, Average Loss: 0.0218\n",
            "Epoch 23/25, Average Loss: 0.0175\n",
            "Epoch 24/25, Average Loss: 0.0187\n",
            "Epoch 25/25, Average Loss: 0.0150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save_pretrained(\"bert-ner-finetuned\")\n",
        "tokenizer.save_pretrained(\"bert-ner-finetuned\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pruFqBu9T1kY",
        "outputId": "dd677d52-388d-4764-ab42-9923349bd553"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('bert-ner-finetuned/tokenizer_config.json',\n",
              " 'bert-ner-finetuned/special_tokens_map.json',\n",
              " 'bert-ner-finetuned/vocab.txt',\n",
              " 'bert-ner-finetuned/added_tokens.json',\n",
              " 'bert-ner-finetuned/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, model, tokenizer, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    # Tokenize the text\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_offsets_mapping=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    offset_mapping = tokenized.offset_mapping[0].tolist()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_ids = tokenized[\"input_ids\"].to(device)\n",
        "        attention_mask = tokenized[\"attention_mask\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "    # Get predictions for each token\n",
        "    predictions = predictions[0].cpu().numpy()\n",
        "    offset_mapping = tokenized[\"offset_mapping\"][0].numpy()\n",
        "\n",
        "    # Initialize list to store entities\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "\n",
        "    for idx, (pred, (start_char, end_char)) in enumerate(zip(predictions, offset_mapping)):\n",
        "        # Skip special tokens ([CLS], [SEP], padding)\n",
        "        if start_char == 0 and end_char == 0:\n",
        "            continue\n",
        "\n",
        "        pred_label = id2label[pred]\n",
        "\n",
        "        # If we find a B- tag, start a new entity\n",
        "        if pred_label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "            entity_text = text[start_char:end_char]\n",
        "            current_entity = {\n",
        "                \"type\": pred_label[2:],\n",
        "                \"start\": start_char,\n",
        "                \"end\": end_char,\n",
        "                \"text\": entity_text\n",
        "            }\n",
        "        # If we find an I- tag, append to the current entity\n",
        "        elif pred_label.startswith(\"I-\") and current_entity:\n",
        "            current_entity[\"end\"] = end_char\n",
        "            current_entity[\"text\"] = text[current_entity[\"start\"]:end_char]\n",
        "        # If we find an O tag, close the current entity\n",
        "        elif pred_label == \"O\" and current_entity:\n",
        "            entities.append(current_entity)\n",
        "            current_entity = None\n",
        "\n",
        "    # Add the last entity if it exists\n",
        "    if current_entity:\n",
        "        entities.append(current_entity)\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Example usage:\n",
        "def test_prediction(model, tokenizer, text):\n",
        "    print(f\"\\nInput text: {text}\")\n",
        "    entities = predict(text, model, tokenizer, device='cpu')\n",
        "    print(\"\\nDetected entities:\")\n",
        "    for entity in entities:\n",
        "        print(f\"- {entity['type']}: '{entity['text']}' (positions {entity['start']}:{entity['end']})\")\n"
      ],
      "metadata": {
        "id": "kNgeyjHfT1qU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model = BertForTokenClassification.from_pretrained(\"bert-ner-finetuned\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-ner-finetuned\")\n",
        "\n",
        "# Test with various examples\n",
        "test_texts = [\n",
        "    \"John works at Google in New York\",\n",
        "    \"Microsoft CEO Satya Nadella visited Seattle\",\n",
        "    \"Apple announced new products in California yesterday\",\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    test_prediction(model, tokenizer, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EY3UemJT1uX",
        "outputId": "73ea26e0-9e3d-4c82-d751-834186debc04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input text: John works at Google in New York\n",
            "\n",
            "Detected entities:\n",
            "- PER: 'John' (positions 0:4)\n",
            "- ORG: 'Google' (positions 14:20)\n",
            "- LOC: 'New York' (positions 24:32)\n",
            "\n",
            "Input text: Microsoft CEO Satya Nadella visited Seattle\n",
            "\n",
            "Detected entities:\n",
            "- ORG: 'Microsoft' (positions 0:9)\n",
            "\n",
            "Input text: Apple announced new products in California yesterday\n",
            "\n",
            "Detected entities:\n",
            "- ORG: 'Apple' (positions 0:5)\n"
          ]
        }
      ]
    }
  ]
}